#!/usr/bin/env python3
"""
VASPç§‘ç ”è®¡ç®—å·¥ä½œæµç¨‹è„šæœ¬
VASP Scientific Computing Workflow Script
ç”¨äºè¿æ¥Claude Codeã€Kimi LLMå’ŒHPCçš„å®Œæ•´ç§‘ç ”è®¡ç®—æµæ°´çº¿
"""

import asyncio
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
from dataclasses import dataclass
import yaml
from dotenv import load_dotenv

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('src')

from vasp_orchestrator import create_vasp_agent
from src.conversation_manager import ConversationManager
from src.hpc_automation import HPCAutomation


@dataclass
class ResearchRequest:
    """ç§‘ç ”éœ€æ±‚æ ‡å‡†æ ¼å¼"""
    scientific_problem: str
    material_system: str
    properties_of_interest: str
    calculation_goals: str
    constraints: Optional[str] = None
    user_request: Optional[str] = None  # åŸå§‹ç”¨æˆ·è¯·æ±‚


@dataclass
class VASPJobSpec:
    """VASPä½œä¸šæ ‡å‡†æ ¼å¼"""
    job_id: str
    analysis_summary: str
    calculation_plan: str
    vasp_parameters: Dict[str, Any]
    hpc_requirements: Dict[str, Any]
    estimated_runtime: str
    success_criteria: str
    incar_content: str
    kpoints_content: str
    poscar_source: str
    potcar_sequence: list
    slurm_script: str


class VASPResearchWorkflow:
    """VASPç§‘ç ”è®¡ç®—å·¥ä½œæµç¨‹ç®¡ç†å™¨"""

    def __init__(self, config_path: str = "config/workflow_config.yaml"):
        self.config = self._load_config(config_path)
        self.workflow_dir = Path("vasp_workflow_jobs")
        self.workflow_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–ä»£ç†
        self.vasp_agent = create_vasp_agent()
        self.conversation_manager = ConversationManager("config/system_prompts.yaml")

        # åˆå§‹åŒ–HPCè‡ªåŠ¨åŒ–
        self.hpc_automation = HPCAutomation(config_path)

        # å·¥ä½œæµç¨‹çŠ¶æ€
        self.current_job = None
        self.workflow_log = []

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½å·¥ä½œæµç¨‹é…ç½®"""
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)

    def _log_workflow(self, step: str, message: str, data: Any = None):
        """è®°å½•å·¥ä½œæµç¨‹æ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "message": message,
            "data": data
        }
        self.workflow_log.append(log_entry)
        print(f"ğŸ”„ [{step}] {message}")

    def _save_workflow_log(self, job_id: str):
        """ä¿å­˜å·¥ä½œæµç¨‹æ—¥å¿—"""
        log_file = self.workflow_dir / f"{job_id}_workflow.json"
        with open(log_file, 'w') as f:
            json.dump(self.workflow_log, f, indent=2)

    async def step1_analyze_research_request(self, user_request: str) -> ResearchRequest:
        """æ­¥éª¤1: åˆ†æç§‘ç ”éœ€æ±‚"""
        self._log_workflow("step1", "å¼€å§‹åˆ†æç§‘ç ”éœ€æ±‚", {"user_request": user_request})

        # ä½¿ç”¨Kimiåˆ†æç§‘ç ”éœ€æ±‚ï¼Œç”Ÿæˆæ ‡å‡†æ ¼å¼
        analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç§‘ç ”éœ€æ±‚ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶æŒ‰æ ‡å‡†æ ¼å¼è¿”å›ï¼š

ç”¨æˆ·éœ€æ±‚: {user_request}

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- scientific_problem: ç§‘å­¦é—®é¢˜çš„å…·ä½“æè¿°
- material_system: ææ–™ä½“ç³»ï¼ˆå¦‚SiCã€çŸ³å¢¨çƒ¯ç­‰ï¼‰
- properties_of_interest: å…³æ³¨çš„æ€§è´¨ï¼ˆå¦‚èƒ½å¸¦ç»“æ„ã€åŠ›å­¦æ€§è´¨ç­‰ï¼‰
- calculation_goals: è®¡ç®—ç›®æ ‡ï¼ˆå¦‚å‡ ä½•ä¼˜åŒ–ã€ç”µå­ç»“æ„è®¡ç®—ç­‰ï¼‰
- constraints: çº¦æŸæ¡ä»¶ï¼ˆå¦‚è®¡ç®—èµ„æºé™åˆ¶ã€ç²¾åº¦è¦æ±‚ç­‰ï¼‰
- analysis_brief: ç®€è¦åˆ†æï¼ˆ50å­—ä»¥å†…ï¼‰

ç¡®ä¿åˆ†æå‡†ç¡®ï¼Œä¸ºåç»­VASPè®¡ç®—æä¾›æ¸…æ™°çš„æŒ‡å¯¼ã€‚
"""

        try:
            result = self.conversation_manager.chat(
                input_text=analysis_prompt,
                system_prompt="ä½ æ˜¯ææ–™ç§‘å­¦ä¸“å®¶ï¼Œæ“…é•¿åˆ†æç§‘ç ”éœ€æ±‚å¹¶åˆ¶å®šè®¡ç®—æ–¹æ¡ˆã€‚",
                temperature=0.3
            )

            if result["status"] == "success":
                # æå–JSONå“åº”
                import re
                json_match = re.search(r'\{.*\}', result["response"], re.DOTALL)
                if json_match:
                    try:
                        analysis_data = json.loads(json_match.group())
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ JSONè§£æé”™è¯¯: {e}")
                        print(f"åŸå§‹å“åº”: {result['response']}")
                        # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
                        json_str = json_match.group()
                        json_str = json_str.replace('\n', '\\n').replace('\r', '\\r')
                        analysis_data = json.loads(json_str)

                    research_request = ResearchRequest(
                        scientific_problem=analysis_data.get("scientific_problem", ""),
                        material_system=analysis_data.get("material_system", ""),
                        properties_of_interest=analysis_data.get("properties_of_interest", ""),
                        calculation_goals=analysis_data.get("calculation_goals", ""),
                        constraints=analysis_data.get("constraints"),
                        user_request=user_request
                    )

                    self._log_workflow("step1", "ç§‘ç ”éœ€æ±‚åˆ†æå®Œæˆ", analysis_data)
                    return research_request
                else:
                    raise ValueError("AIå“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆJSON")
            else:
                raise RuntimeError(f"AIåˆ†æå¤±è´¥: {result.get('error')}")

        except Exception as e:
            self._log_workflow("step1", f"ç§‘ç ”éœ€æ±‚åˆ†æå¤±è´¥: {e}")
            raise

    async def step2_generate_vasp_plan(self, research_request: ResearchRequest) -> VASPJobSpec:
        """æ­¥éª¤2: ç”ŸæˆVASPè®¡ç®—æ–¹æ¡ˆ"""
        self._log_workflow("step2", "å¼€å§‹ç”ŸæˆVASPè®¡ç®—æ–¹æ¡ˆ")

        # æ„å»ºè¯¦ç»†çš„VASPè®¡ç®—æç¤º
        vasp_prompt = f"""
åŸºäºä»¥ä¸‹ç§‘ç ”éœ€æ±‚ï¼Œè¯·ç”Ÿæˆå®Œæ•´çš„VASPè®¡ç®—æ–¹æ¡ˆï¼š

ç§‘ç ”é—®é¢˜: {research_request.scientific_problem}
ææ–™ä½“ç³»: {research_request.material_system}
å…³æ³¨æ€§è´¨: {research_request.properties_of_interest}
è®¡ç®—ç›®æ ‡: {research_request.calculation_goals}
çº¦æŸæ¡ä»¶: {research_request.constraints or "æ— "}

è¯·ç”ŸæˆJSONæ ¼å¼çš„VASPè®¡ç®—æ–¹æ¡ˆï¼ŒåŒ…å«ï¼š

1. analysis_summary: è®¡ç®—æ–¹æ¡ˆæ¦‚è¿°ï¼ˆ100å­—ä»¥å†…ï¼‰
2. calculation_plan: è¯¦ç»†çš„è®¡ç®—æ­¥éª¤å’Œé€»è¾‘
3. vasp_parameters:
   - incar: å®Œæ•´çš„INCARå‚æ•°è®¾ç½®
   - kpoints: Kç‚¹è®¾ç½®æ–¹æ¡ˆ
   - poscar_source: POSCARæ–‡ä»¶æ¥æºè¯´æ˜
   - potcar_sequence: POTCARå…ƒç´ é¡ºåº

4. hpc_requirements:
   - nodes: èŠ‚ç‚¹æ•°
   - ntasks_per_node: æ¯èŠ‚ç‚¹ä»»åŠ¡æ•°
   - walltime: é¢„ä¼°è®¡ç®—æ—¶é—´
   - partition: è®¡ç®—åˆ†åŒº

5. estimated_runtime: æ€»é¢„ä¼°è¿è¡Œæ—¶é—´
6. success_criteria: è®¡ç®—æˆåŠŸçš„åˆ¤æ–­æ ‡å‡†

è¦æ±‚ï¼š
- å‚æ•°è®¾ç½®è¦ç§‘å­¦åˆç†ï¼Œç¬¦åˆææ–™è®¡ç®—æœ€ä½³å®è·µ
- è€ƒè™‘HPCèµ„æºæ•ˆç‡
- ç¡®ä¿è®¡ç®—æ”¶æ•›æ€§å’Œç²¾åº¦
- æä¾›å®Œæ•´çš„æŠ€æœ¯å‚æ•°

è¯·ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""

        try:
            result = self.conversation_manager.chat(
                input_text=vasp_prompt,
                system_prompt="ä½ æ˜¯VASPè®¡ç®—ä¸“å®¶ï¼Œè¯·ç”Ÿæˆä¸“ä¸šã€å®Œæ•´çš„VASPè®¡ç®—æ–¹æ¡ˆã€‚",
                temperature=0.2
            )

            if result["status"] == "success":
                # æå–JSONå“åº”
                import re
                json_match = re.search(r'\{.*\}', result["response"], re.DOTALL)
                if json_match:
                    try:
                        vasp_data = json.loads(json_match.group())
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ JSONè§£æé”™è¯¯: {e}")
                        print(f"åŸå§‹å“åº”ç‰‡æ®µ: {result['response'][:500]}...")
                        # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
                        json_str = json_match.group()
                        json_str = json_str.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                        vasp_data = json.loads(json_str)

                    # ç”Ÿæˆå”¯ä¸€çš„ä½œä¸šID
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    job_id = f"{research_request.material_system}_{timestamp}"

                    # åˆ›å»ºVASPä½œä¸šè§„èŒƒ
                    job_spec = VASPJobSpec(
                        job_id=job_id,
                        analysis_summary=vasp_data.get("analysis_summary", ""),
                        calculation_plan=vasp_data.get("calculation_plan", ""),
                        vasp_parameters=vasp_data.get("vasp_parameters", {}),
                        hpc_requirements=vasp_data.get("hpc_requirements", {}),
                        estimated_runtime=vasp_data.get("estimated_runtime", ""),
                        success_criteria=vasp_data.get("success_criteria", ""),
                        incar_content=self._generate_incar_content(vasp_data.get("vasp_parameters", {}).get("incar", {})),
                        kpoints_content=self._generate_kpoints_content(vasp_data.get("vasp_parameters", {}).get("kpoints", {})),
                        poscar_source=vasp_data.get("vasp_parameters", {}).get("poscar_source", ""),
                        potcar_sequence=vasp_data.get("vasp_parameters", {}).get("potcar_sequence", []),
                        slurm_script=self._generate_slurm_content(job_id, vasp_data.get("hpc_requirements", {}))
                    )

                    self.current_job = job_spec
                    self._log_workflow("step2", "VASPè®¡ç®—æ–¹æ¡ˆç”Ÿæˆå®Œæˆ", {"job_id": job_id})
                    return job_spec
                else:
                    raise ValueError("AIå“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆJSON")
            else:
                raise RuntimeError(f"VASPæ–¹æ¡ˆç”Ÿæˆå¤±è´¥: {result.get('error')}")

        except Exception as e:
            self._log_workflow("step2", f"VASPè®¡ç®—æ–¹æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            raise

    def _generate_incar_content(self, incar_params: Dict[str, Any]) -> str:
        """ç”ŸæˆINCARæ–‡ä»¶å†…å®¹"""
        lines = ["VASP INCAR file", "=" * 20]
        for key, value in incar_params.items():
            lines.append(f"{key} = {value}")
        return "\n".join(lines) + "\n"

    def _generate_kpoints_content(self, kpoints_params: Dict[str, Any]) -> str:
        """ç”ŸæˆKPOINTSæ–‡ä»¶å†…å®¹"""
        if "content" in kpoints_params:
            return kpoints_params["content"]

        mode = kpoints_params.get("mode", "Monkhorst-Pack")
        if "grid" in kpoints_params:
            grid = kpoints_params["grid"]
            return f"""Automatic mesh
0
{mode}
{grid[0]} {grid[1]} {grid[2]} 0 0 0
"""
        else:
            return "KPOINTS file (content should be provided by AI)"

    def _generate_slurm_content(self, job_id: str, hpc_params: Dict[str, Any]) -> str:
        """ç”ŸæˆSlurmè„šæœ¬å†…å®¹"""
        default_hpc = self.config["hpc_environment"]["default_resources"]

        nodes = hpc_params.get("nodes", default_hpc["nodes"])
        ntasks = hpc_params.get("ntasks_per_node", default_hpc["ntasks_per_node"])
        walltime = hpc_params.get("walltime", default_hpc["walltime"])
        partition = hpc_params.get("partition", default_hpc["partition"])

        return f"""#!/bin/bash
#SBATCH -p {partition}
#SBATCH -N {nodes}
#SBATCH --ntasks-per-node={ntasks}
#SBATCH -J {job_id}
#SBATCH -o vasp.out
#SBATCH -t {walltime}

module load {self.config["hpc_environment"]["vasp_module"]["name"]}
export VASP_PP_PATH={self.config["hpc_environment"]["vasp_module"]["potcar_path"]}

echo "Starting VASP calculation for {job_id}"
echo "Start time: $(date)"
echo "Job ID: $SLURM_JOB_ID"
echo "Nodes: $SLURM_JOB_NUM_NODES"
echo "Tasks: $SLURM_NTASKS"

# Build POTCAR (example - should be customized based on elements)
# cat $VASP_PP_PATH/*/POTCAR > POTCAR

mpirun -np $SLURM_NTASKS {self.config["hpc_environment"]["vasp_module"]["executable"]}

echo "VASP calculation completed"
echo "End time: $(date)"
"""

    def step3_prepare_vasp_files(self, job_spec: VASPJobSpec) -> str:
        """æ­¥éª¤3: å‡†å¤‡VASPè¾“å…¥æ–‡ä»¶"""
        self._log_workflow("step3", "å¼€å§‹å‡†å¤‡VASPè¾“å…¥æ–‡ä»¶")

        # åˆ›å»ºä½œä¸šç›®å½•
        job_dir = self.workflow_dir / job_spec.job_id
        job_dir.mkdir(exist_ok=True)

        # ç”Ÿæˆè¾“å…¥æ–‡ä»¶
        files_created = {}

        # INCAR
        incar_path = job_dir / "INCAR"
        with open(incar_path, 'w') as f:
            f.write(job_spec.incar_content)
        files_created["INCAR"] = str(incar_path)

        # KPOINTS
        kpoints_path = job_dir / "KPOINTS"
        with open(kpoints_path, 'w') as f:
            f.write(job_spec.kpoints_content)
        files_created["KPOINTS"] = str(kpoints_path)

        # Slurmè„šæœ¬
        slurm_path = job_dir / "run.slurm"
        with open(slurm_path, 'w') as f:
            f.write(job_spec.slurm_script)
        files_created["run.slurm"] = str(slurm_path)

        # ä¿å­˜ä½œä¸šè§„èŒƒ
        job_spec_path = job_dir / "job_specification.json"
        with open(job_spec_path, 'w') as f:
            json.dump(job_spec.__dict__, f, indent=2, default=str)
        files_created["job_specification"] = str(job_spec_path)

        self._log_workflow("step3", "VASPè¾“å…¥æ–‡ä»¶å‡†å¤‡å®Œæˆ", files_created)
        return str(job_dir)

    def step4_test_hpc_connection(self) -> bool:
        """æ­¥éª¤4: æµ‹è¯•HPCè¿æ¥"""
        self._log_workflow("step4", "æµ‹è¯•HPCè¿æ¥")

        success = self.hpc_automation.test_hpc_connection()
        if success:
            self._log_workflow("step4", "HPCè¿æ¥æµ‹è¯•æˆåŠŸ")
        else:
            self._log_workflow("step4", "HPCè¿æ¥æµ‹è¯•å¤±è´¥")

        return success

    def step5_upload_and_submit(self, job_spec: VASPJobSpec) -> Optional[str]:
        """æ­¥éª¤5: ä¸Šä¼ æ–‡ä»¶å¹¶æäº¤ä½œä¸š"""
        self._log_workflow("step5", "å¼€å§‹ä¸Šä¼ æ–‡ä»¶å¹¶æäº¤HPCä½œä¸š")

        try:
            # ä¸Šä¼ æ–‡ä»¶
            if not self.hpc_automation.upload_job_files(self.workflow_dir / job_spec.job_id, job_spec.job_id):
                raise Exception("æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
            self._log_workflow("step5", "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")

            # æäº¤ä½œä¸š
            slurm_job_id = self.hpc_automation.submit_vasp_job(job_spec.job_id)
            if not slurm_job_id:
                raise Exception("ä½œä¸šæäº¤å¤±è´¥")

            self._log_workflow("step5", f"ä½œä¸šæäº¤æˆåŠŸï¼ŒSlurm ID: {slurm_job_id}")
            return slurm_job_id

        except Exception as e:
            self._log_workflow("step5", f"HPCä½œä¸šæäº¤å¤±è´¥: {e}")
            return None

    def step6_monitor_job(self, slurm_job_id: str) -> bool:
        """æ­¥éª¤6: ç›‘æ§ä½œä¸šæ‰§è¡Œ"""
        self._log_workflow("step6", f"å¼€å§‹ç›‘æ§ä½œä¸š: {slurm_job_id}")

        success = self.hpc_automation.monitor_job(slurm_job_id, check_interval=30, max_wait=300)  # çŸ­æ—¶é—´æµ‹è¯•
        if success:
            self._log_workflow("step6", "ä½œä¸šæ‰§è¡ŒæˆåŠŸ")
        else:
            self._log_workflow("step6", "ä½œä¸šæ‰§è¡Œå¤±è´¥æˆ–è¶…æ—¶")

        return success

    async def run_complete_workflow(self, user_request: str) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ç§‘ç ”è®¡ç®—å·¥ä½œæµç¨‹"""
        self._log_workflow("workflow_start", "å¼€å§‹VASPç§‘ç ”è®¡ç®—å·¥ä½œæµç¨‹")

        try:
            # æ­¥éª¤1: åˆ†æç§‘ç ”éœ€æ±‚
            research_request = await self.step1_analyze_research_request(user_request)

            # æ­¥éª¤2: ç”ŸæˆVASPè®¡ç®—æ–¹æ¡ˆ
            job_spec = await self.step2_generate_vasp_plan(research_request)

            # æ­¥éª¤3: å‡†å¤‡VASPæ–‡ä»¶
            job_dir = self.step3_prepare_vasp_files(job_spec)

            # æ­¥éª¤4: æµ‹è¯•HPCè¿æ¥
            hpc_connection_ok = self.step4_test_hpc_connection()
            if not hpc_connection_ok:
                raise Exception("HPCè¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")

            # æ­¥éª¤5: ä¸Šä¼ æ–‡ä»¶å¹¶æäº¤ä½œä¸š
            slurm_job_id = self.step5_upload_and_submit(job_spec)
            if not slurm_job_id:
                raise Exception("HPCä½œä¸šæäº¤å¤±è´¥")

            # ä¿å­˜å·¥ä½œæµç¨‹æ—¥å¿—
            self._save_workflow_log(job_spec.job_id)

            result = {
                "status": "success",
                "job_id": job_spec.job_id,
                "slurm_job_id": slurm_job_id,
                "job_directory": job_dir,
                "research_request": research_request.__dict__,
                "job_specification": job_spec.__dict__,
                "hpc_status": "submitted",
                "next_steps": [
                    f"ç›‘æ§ä½œä¸šçŠ¶æ€ (Slurm ID: {slurm_job_id})",
                    "ç­‰å¾…è®¡ç®—å®Œæˆ",
                    "ä¸‹è½½è®¡ç®—ç»“æœ",
                    "åˆ†æè®¡ç®—æ•°æ®"
                ]
            }

            self._log_workflow("workflow_complete", "å·¥ä½œæµç¨‹å®Œæˆï¼ŒHPCä½œä¸šå·²æäº¤")
            return result

        except Exception as e:
            error_result = {
                "status": "error",
                "error": str(e),
                "workflow_log": self.workflow_log
            }
            self._log_workflow("workflow_error", f"å·¥ä½œæµç¨‹å¤±è´¥: {e}")
            return error_result

    async def run_workflow_without_hpc(self, user_request: str) -> Dict[str, Any]:
        """è¿è¡Œä¸åŒ…å«HPCæäº¤çš„å·¥ä½œæµç¨‹ï¼ˆä»…ç”Ÿæˆæ–‡ä»¶ï¼‰"""
        self._log_workflow("workflow_start", "å¼€å§‹VASPç§‘ç ”è®¡ç®—å·¥ä½œæµç¨‹ï¼ˆä»…ç”Ÿæˆæ–‡ä»¶ï¼‰")

        try:
            # æ­¥éª¤1: åˆ†æç§‘ç ”éœ€æ±‚
            research_request = await self.step1_analyze_research_request(user_request)

            # æ­¥éª¤2: ç”ŸæˆVASPè®¡ç®—æ–¹æ¡ˆ
            job_spec = await self.step2_generate_vasp_plan(research_request)

            # æ­¥éª¤3: å‡†å¤‡VASPæ–‡ä»¶
            job_dir = self.step3_prepare_vasp_files(job_spec)

            # ä¿å­˜å·¥ä½œæµç¨‹æ—¥å¿—
            self._save_workflow_log(job_spec.job_id)

            result = {
                "status": "success",
                "job_id": job_spec.job_id,
                "job_directory": job_dir,
                "research_request": research_request.__dict__,
                "job_specification": job_spec.__dict__,
                "next_steps": [
                    "æ£€æŸ¥ç”Ÿæˆçš„VASPè¾“å…¥æ–‡ä»¶",
                    "ç¡®è®¤è®¡ç®—å‚æ•°è®¾ç½®",
                    "æ‰‹åŠ¨å‡†å¤‡HPCæäº¤",
                    "ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æäº¤ä½œä¸š"
                ],
                "manual_submit_commands": [
                    f"python src/hpc_automation.py test",
                    f"python src/hpc_automation.py run {job_spec.job_id}"
                ]
            }

            self._log_workflow("workflow_complete", "æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œå‡†å¤‡æ‰‹åŠ¨HPCæäº¤")
            return result

        except Exception as e:
            error_result = {
                "status": "error",
                "error": str(e),
                "workflow_log": self.workflow_log
            }
            self._log_workflow("workflow_error", f"å·¥ä½œæµç¨‹å¤±è´¥: {e}")
            return error_result


# å‘½ä»¤è¡Œæ¥å£
async def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œè°ƒç”¨"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python vasp_research_workflow.py 'ç§‘ç ”éœ€æ±‚æè¿°'")
        print("ç¤ºä¾‹: python vasp_research_workflow.py 'ç ”ç©¶SiCçš„èƒ½å¸¦ç»“æ„å’Œå…‰å­¦æ€§è´¨'")
        sys.exit(1)

    user_request = " ".join(sys.argv[1:])

    # ç¡®ä¿ç¯å¢ƒå˜é‡å·²åŠ è½½
    load_dotenv()

    if not os.getenv("KIMI_API_KEY"):
        print("âŒ é”™è¯¯: KIMI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        sys.exit(1)

    print("ğŸš€ å¯åŠ¨VASPç§‘ç ”è®¡ç®—å·¥ä½œæµç¨‹...")
    print(f"ğŸ“‹ ç§‘ç ”éœ€æ±‚: {user_request}")
    print("=" * 60)

    workflow = VASPResearchWorkflow()
    result = await workflow.run_complete_workflow(user_request)

    if result["status"] == "success":
        print("\nâœ… å·¥ä½œæµç¨‹å®Œæˆ!")
        print(f"ğŸ“ ä½œä¸šç›®å½•: {result['job_directory']}")
        print(f"ğŸ†” ä½œä¸šID: {result['job_id']}")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        for i, step in enumerate(result['next_steps'], 1):
            print(f"  {i}. {step}")
    else:
        print(f"\nâŒ å·¥ä½œæµç¨‹å¤±è´¥: {result['error']}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())